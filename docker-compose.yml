version: "3.9"

services:

####################
# Admin
####################
  
  homepage:
    image: ghcr.io/benphelps/homepage:latest
    container_name: homepage
    profiles:
      - admin

    networks:
      - socky_proxy-net

    ports:
      - ${PORT_DASH_HTTP}:3000

    environment:
      PUID: ${PUID}
      PGID: ${PGID}
      TZ: ${TZ}

    volumes:
      - ${CONFIGDIR}/homepage:/app/config # Make sure your local config directory exists
      - ${STATICDIR}/icons:/app/public/icons #Optional. for custom icons
      - ${STATICDIR}/wallpaper:/app/public/images #Optional. for custom backgrounds

      - ${SYS_DISK1}:/dev/sda1:ro #harddrive space monitoring
      - ${SYS_DISK2}:/dev/sdb1:ro #harddrive space monitoring
      - ${SYS_DISK3}:/dev/sdc1:ro #harddrive space monitoring
    
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 128M
    restart: unless-stopped

  uptime-kuma:
    container_name: uptime_kuma
    image: louislam/uptime-kuma
    profiles:
      - admin
    
    networks:
      - caddy-net # provides access for the reverse proxy
    
    ports:
      - ${PORT_UPKUMA}:3001
    
    labels:
      - diun.enable=true
      - homepage.group=System
      - homepage.name=Uptime Kuma
      - homepage.icon=uptime-kuma
      - homepage.href=http://${SERVER_URL}:${PORT_UPKUMA}
      - homepage.description=Service Uptime Monitoring
      - homepage.widget.type=uptimekuma
      - homepage.widget.url=http://${SERVER_URL}:${PORT_UPKUMA}
      - homepage.widget.slug=statuspageslug

    volumes:
      - ${CONFIGDIR}/uptime-kuma:/app/data
    
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 128M
    restart: always

####################
# Recipes
####################

  tandoor:
    container_name: tandoor
    image: vabene1111/recipes:latest
    profiles:
      - recipes

    depends_on:
      tandoor_db:
        condition: service_healthy
    
    networks:
      - caddy-net # provides access for the reverse proxy
      - tandoor-net

    ports:
      - ${PORT_TANDOOR}:8080

    env_file:
      - ${STATICCONFIGDIR}/tandoor/tandoor.env

    environment:
      TIMEZONE: ${TZ}

    labels:
      - diun.enable=true
      - homepage.group=Lifestyle
      - homepage.name=Tandoor
      - homepage.icon=tandoorrecipes
      - homepage.href=http://${SERVER_URL}:${PORT_TANDOOR}
      - homepage.description=Recipes and Meal Planning

    volumes:
      - ${STATICDIR}/tandoor_media:/opt/recipes/mediafiles

    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 512M
    restart: unless-stopped

  tandoor_db:
    container_name: tandoor_db
    image: postgres:alpine

    profiles:
      - recipes

    networks:
      - tandoor-net

    env_file:
      - ${STATICCONFIGDIR}/tandoor/tandoor.env

    labels:
      - homepage.group=Back End
      - homepage.name=Tandoor | Postgres DB
      - homepage.icon=postgres
      - homepage.description=Database for Tandoor recipes

    volumes:
      - ${DBDIR}/tandoor_db:/var/lib/postgresql/data

    healthcheck:
      interval: 60s
      retries: 10
      test:
        - CMD
        - pg_isready
        - '-q'
        - '-d'
        - ${TANDOOR_DB_NAME}
        - '-U'
        - ${TANDOOR_DB_USER}
      timeout: 45s
      
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 128M
    restart: unless-stopped

####################
# Media
####################

  jellyfin:
    container_name: jellyfin
    image: jellyfin/jellyfin
    profiles:
      - media
      - media-request
      
    user: ${PUID}:${PGID}
    group_add:
      - ${GID_HARDWAREACC} # gid of your `render` group

    networks:
      - htpc-net
      - caddy-net # provides access for the reverse proxy
    
    ports:
      - ${PORT_HTPC_HTTP}:8096 #HTTP webUI
      - ${PORT_HTPC_HTTPS}:8920 #HTTPS webUI
      - ${PORT_HTPC_LOCAL}:7359/udp #optional. Allows clients to discover Jellyfin on the local network.
      - ${PORT_HTPC_DLNA}:1900/udp #optional. Service discovery used by DNLA and clients.

    environment:
      TZ: ${TZ}
      JELLYFIN_PublishedServerUrl: ${SERVER_URL}:${PORT_HTPC_HTTP} #optional. The Server URL to publish in udp Auto Discovery response.

    labels:
      - diun.enable=true
      - homepage.group=Media
      - homepage.name=Jellyfin
      - homepage.icon=jellyfin
      - homepage.href=http://${SERVER_URL}:${PORT_HTPC_HTTP}
      - homepage.description=Media Streaming Server
      - homepage.widget.type=jellyfin
      - homepage.widget.url=http://${SERVER_URL}:${PORT_HTPC_HTTP}
      - homepage.widget.key=${HOMEPAGE_JELLYFIN_API}

    devices:
      - /dev/dri/renderD128:/dev/dri/renderD128 # VAAPI
      - /dev/dri/card0:/dev/dri/card0

    volumes:
      - ${CONFIGDIR}/jellyfin:/config
      - ${CACHEDIR}/jellyfin:/cache
      - ${MEDIADIR}:/media:ro #naming, etc all handled by *arr apps
    
    #deploy:
    #  resources:
    #    limits:
    #      cpus: '2'
    #      memory: 1024M
    restart: unless-stopped
  
  jellyseerr:
    container_name: jellyseerr
    image: fallenbagel/jellyseerr:latest
    profiles:
      - media-request

    depends_on:
      - sonarr
      - radarr
      - jellyfin # for authentication

    networks:
      - htpc-net
      - caddy-net # provides access for the reverse proxy

    ports:
      - ${PORT_HTPC_REQUESTS}:5055

    environment:
      LOG_LEVEL: ${LOG_LEVEL}
      TZ: ${TZ}

    labels:
      - diun.enable=true
      - homepage.group=Media
      - homepage.name=Jellyseerr
      - homepage.icon=jellyseerr
      - homepage.href=http://${SERVER_URL}:${PORT_HTPC_REQUESTS}
      - homepage.description=Request Movies and TV Shows
      - homepage.widget.type=jellyseerr
      - homepage.widget.url=http://${SERVER_URL}:${PORT_HTPC_REQUESTS}
      - homepage.widget.key=${HOMEPAGE_JELLYSEERR_API}

    volumes:
        - ${CONFIGDIR}/jellyseerr:/app/config
    
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 512M
    restart: unless-stopped
  
  qbittorrent:
    container_name: qbittorrent
    image: ghcr.io/linuxserver/qbittorrent
    profiles:
      - downloads
      - media-request

    depends_on:
      protonvpn:
        condition: service_healthy

    network_mode: service:protonvpn # ONLY provide network config in the protonvpn container

    environment:
        PUID: ${PUID}
        PGID: ${PGID}
        TZ: ${TZ}
        UMASK_SET: 022
        WEBUI_PORT: ${PORT_TORRENT_UI}

    labels:
      - diun.enable=true
      - homepage.group=Downloads
      - homepage.name=qBittorrent
      - homepage.icon=qbittorrent
      - homepage.href=http://${SERVER_URL}:${PORT_TORRENT_UI}
      - homepage.description=Torrent Management
      - homepage.widget.type=qbittorrent
      - homepage.widget.url=http://${SERVER_URL}:${PORT_TORRENT_UI}
      - homepage.widget.username=${HOMEPAGE_QBITTORENT_USERNAME}
      - homepage.widget.password=${HOMEPAGE_QBITTORENT_PASSWORD}

    volumes:
        - ${CONFIGDIR}/qbt:/config
        - ${DOWNLOADDIR}:/data/downloads
    
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 128M
    healthcheck:
      test: ls /sys/class/net/ | grep -o "wg[0-9*]" > /dev/null || exit 1 # VPN Killswitch. Check to see if the wg0 interface is available, or fail. the protonvpn container supplies wg0.
      interval: 30s
      timeout: 6s
      retries: 3
      start_period: 20s
    restart: unless-stopped

  prowlarr:
    container_name: prowlarr
    image: lscr.io/linuxserver/prowlarr:develop
    profiles:
      - media-request

    depends_on:
      protonvpn:
        condition: service_healthy
      qbittorrent:
        condition: service_started

    network_mode: service:protonvpn # ONLY provide network config in the protonvpn container

    environment:
      PUID: ${PUID}
      PGID: ${PGID}
      TZ: ${TZ}

    labels:
      - diun.enable=true
      - homepage.group=Downloads
      - homepage.name=Prowlarr
      - homepage.icon=prowlarr
      - homepage.href=http://${SERVER_URL}:${PORT_PROWLARR_UI}
      - homepage.description=Indexer manager/proxy
      - homepage.widget.type=prowlarr
      - homepage.widget.url=http://${SERVER_URL}:${PORT_PROWLARR_UI}
      - homepage.widget.key=${HOMEPAGE_PROWLARR_API}
      
    volumes:
      - ${CONFIGDIR}/prowlarr:/config

    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 500M
    restart: unless-stopped
  
  radarr:
    container_name: radarr
    image: ghcr.io/linuxserver/radarr:latest
    profiles:
      - media-request
    
    depends_on:
      - prowlarr

    ports:
      - ${PORT_RADARR_UI}:7878

    environment:
      PUID: ${PUID}
      PGID: ${PGID}
      TZ: ${TZ}

    labels:
      - diun.enable=true
      - homepage.group=Downloads
      - homepage.name=Radarr
      - homepage.icon=radarr
      - homepage.href=http://${SERVER_URL}:${PORT_RADARR_UI}
      - homepage.description=Movie collection manager
      - homepage.widget.type=radarr
      - homepage.widget.url=http://${SERVER_URL}:${PORT_RADARR_UI}
      - homepage.widget.key=${HOMEPAGE_RADARR_API}

    volumes:
        - /etc/localtime:/etc/localtime:ro
        - ${CONFIGDIR}/radarr3:/config
        - ${DATADIR}:/data
    
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 512M
    restart: unless-stopped

  sonarr:
    container_name: sonarr
    image: ghcr.io/linuxserver/sonarr:latest
    profiles:
      - media-request

    depends_on:
      - prowlarr

    ports:
      - ${PORT_SONARR_UI}:8989

    environment:
      PUID: ${PUID}
      PGID: ${PGID}
      TZ: ${TZ}

    labels:
      - diun.enable=true
      - homepage.group=Downloads
      - homepage.name=Sonarr
      - homepage.icon=sonarr
      - homepage.href=http://${SERVER_URL}:${PORT_SONARR_UI}
      - homepage.description=TV Show collection manager
      - homepage.widget.type=sonarr
      - homepage.widget.url=http://${SERVER_URL}:${PORT_SONARR_UI}
      - homepage.widget.key=${HOMEPAGE_SONARR_API}

    volumes:
        - /etc/localtime:/etc/localtime:ro
        - ${CONFIGDIR}/sonarr:/config
        - ${DATADIR}:/data
    
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 512M
    restart: unless-stopped

  podgrab:
    container_name: podgrab
    image: ghcr.io/akhilrex/podgrab
    profiles:
      - media-request
      - downloads

    ports:
      - ${PORT_PODGRAB}:8080

    environment:
      - CHECK_FREQUENCY=1800

    labels:
      - homepage.group=Media
      - homepage.name=Podgrab
      - homepage.href=http://${SERVER_URL}:${PORT_PODGRAB}
      - homepage.description=Podcast manager/downloader/archiver tool to download podcast episodes

    volumes:
        - /etc/localtime:/etc/localtime:ro
        - ${CONFIGDIR}/podgrab:/config
        - ${MEDIADIR}/podcasts:/assets

    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 512M
    restart: unless-stopped

####################
# Automatic Ripping Machine
####################

  automatic-ripping-machine:
    container_name: automatic-ripping-machine
    image: 1337server/automatic-ripping-machine:latest
    profiles:
      - ripping

    privileged: true

    ports:
      - ${PORT_RIPPING}:8080

    environment:
      ARM_UID: ${RIPPING_UID}
      ARM_GID: ${RIPPING_GID}

    labels:
      - homepage.group=Downloads
      - homepage.name=Automatic Ripping Machine
      - homepage.href=http://${SERVER_URL}:${PORT_RIPPING}
      - homepage.description=Disk-in Music/Movie Ripping

    devices:
      - /dev/sr0:/dev/sr0
      
    volumes:
      - ${CONFIGDIR}/ripping:/etc/arm/config
      - /etc/localtime:/etc/localtime:ro

      - ${RIPPING_HOME}:/home/arm
      - ${LOGDIR}/ripping:/home/arm/logs

      - ${DOWNLOADDIR}/rips/video:/home/arm/media/completed
      - ${DOWNLOADDIR}/rips/music:/home/arm/Music


    deploy:
      resources:
        limits:
          cpus: '5'
          memory: 2048M #for a single rip, ARM is not RAM limited.
    restart: 'no'

####################
# Gloomhaven
####################

  gloomhaven:
    container_name: gloomhaven
    image: ghcr.io/lurkars/ghs
    profiles:
      - gloomhaven

    networks:
      - caddy-net

    ports:
      - ${PORT_GHS_CLIENT}:80

    labels:
      - diun.enable=true
      - homepage.group=Games
      - homepage.name=Gloomhaven Secretariat
      - homepage.icon=/icons/ghs.png
      - homepage.href=http://${SERVER_URL}:${PORT_GHS_CLIENT}
      - homepage.description=Gloomhaven Ability and Initiative tracker
      

    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 128M
    restart: unless-stopped

  gloomhaven-server:
    container_name: gloomhaven-server
    image: ghcr.io/lurkars/ghs-server
    profiles:
      - gloomhaven

    networks:
      - caddy-net
      
    ports:
      - ${PORT_GHS_SERVER}:8080

    labels:
      - diun.enable=true
      - homepage.group=Back End
      - homepage.name=Gloomhaven Secretariat Server
      - homepage.icon=/icons/ghs.png
      - homepage.href=http://${SERVER_URL}:${PORT_GHS_SERVER}
      - homepage.description=Backend for GLoomhaven Secretariat

    volumes:
      - ${CONFIGDIR}/ghs:/root/.ghs # optional, to back up Party Sheets etc.

    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
    restart: unless-stopped

####################
# Lifestyle
####################

  changedetection:
    container_name: changedetection
    image: ghcr.io/dgtlmoon/changedetection.io

    profiles:
      - lifestyle
    
    ports:
      - ${PORT_CHANGEDETECTION}:5000

    environment:
      PUID: ${PUID}
      PGID: ${PGID}
      TZ: ${TZ}

    labels:
      - diun.enable=true
      - homepage.group=Lifestyle
      - homepage.name=Change Detection
      - homepage.icon=/icons/changedetection.png
      - homepage.href=http://${SERVER_URL}:${PORT_CHANGEDETECTION}
      - homepage.description=Website monitoring and change detection
      - homepage.widget.type=changedetectionio
      - homepage.widget.url=http://${SERVER_URL}:${PORT_CHANGEDETECTION}
      - homepage.widget.key=${HOMEPAGE_CHANGEDETECTION_API}

    volumes:
      - ${CONFIGDIR}/changedetection:/datastore
    
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 128M
    restart: unless-stopped
  
  pinry:
    container_name: pinry
    image: getpinry/pinry

    profiles:
      - lifestyle

    networks:
      - caddy-net

    ports:
      - ${PORT_PINRY}:80

    environment:
      ALLOW_NEW_REGISTRATIONS: 'True'
      SECRET_KEY: ${PINRY_SECRETKEY}
    
    labels:
      - diun.enable=true
      - homepage.group=Lifestyle
      - homepage.name=Pinry
      - homepage.icon=/icons/pinry.png
      - homepage.href=http://${SERVER_URL}:${PORT_PINRY}
      - homepage.description=Save, tag, and share images, videos and webpages

    volumes:
      - ${CONFIGDIR}/pinry:/data
    
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
    restart: unless-stopped

####################
# Rallly
####################

  rallly:
    container_name: rallly
    image: lukevella/rallly:latest

    profiles:
      - calendar

    networks:
      - caddy-net
      - rallly-net

    depends_on:
      rallly_db:
        condition: service_healthy

    ports:
      - ${PORT_RALLLY}:3000

    environment:
      DATABASE_URL: postgres://${RALLLY_DB_USER}:${RALLLY_DB_PASSWORD}@rallly_db:5432/${RALLLY_DB_NAME}
      SECRET_PASSWORD: ${RALLLY_SECRETKEY}
      AUTH_REQUIRED: true
      DISABLE_LANDING_PAGE: true
      SUPPORT_EMAIL: ${RALLLY_SUPPORTEMAIL}
      NEXT_PUBLIC_BASE_URL: https://meet.${DOMAIN} # defined in the Caddyfile

      SMTP_HOST: ${RALLLY_SMTP_HOST}
      SMTP_PORT:  ${RALLLY_SMTP_PORT}
      SMTP_SECURE: ${RALLLY_SMTP_SECURE}
      SMTP_USER: ${RALLLY_SMTP_USER}
      SMTP_PWD: ${RALLLY_SMTP_PASSWORD}
    
    labels:
      - diun.enable=true
      - homepage.group=Lifestyle
      - homepage.name=Rallly
      - homepage.icon=/icons/rallly.png
      - homepage.href=http://${SERVER_URL}:${PORT_RALLLY}
      - homepage.description=Schedule hangouts with friends

    volumes:
      - ${CONFIGDIR}/rallly:/data
    
    deploy:
      resources:
        limits:
          cpus: '0.7'
          memory: 256M
    restart: unless-stopped

  rallly_db:
    container_name: rallly_db
    image: postgres:alpine

    profiles:
      - calendar

    networks:
      - rallly-net

    ports:
      - ${PORT_RALLLY_DB}:5432

    environment:
      POSTGRES_USER: ${RALLLY_DB_USER}
      POSTGRES_PASSWORD: ${RALLLY_DB_PASSWORD}
      POSTGRES_DB: ${RALLLY_DB_NAME}

    labels:
      - homepage.group=Back End
      - homepage.name=Rallly | Postgres DB
      - homepage.icon=postgres
      - homepage.description=Database for Rallly

    volumes:
      - ${DBDIR}/rallly:/var/lib/postgresql/data

    healthcheck:
      interval: 60s
      retries: 10
      test:
        - CMD
        - pg_isready
        - '-q'
        - '-d'
        - ${RALLLY_DB_NAME}
        - '-U'
        - ${RALLLY_DB_USER}
      timeout: 45s
      
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 128M
    restart: unless-stopped

####################
# Network
####################

  pihole:
  # More info at https://github.com/pi-hole/docker-pi-hole/ and https://docs.pi-hole.net/
    container_name: pihole
    hostname: pihole
    image: pihole/pihole:latest
    profiles:
      - network
    
    depends_on:
      - unbound

    networks:
      dns-net:
        ipv4_address: 172.20.0.2

    # For DHCP it is recommended to remove these ports and instead add: network_mode: "host"
    ports:
      - ${PORT_PIHOLE_WEB:-80}:80
      - ${PORT_PIHOLE_WEBSSL:-443}:443
      - ${SERVER_URL}:${PORT_PIHOLE_TCP:-53}:53/tcp
      - ${SERVER_URL}:${PORT_PIHOLE_UDP:-53}:53/udp
    #  - ${PORT_PIHOLE_DHCP:-67}:67/udp # Only required if you are using Pi-hole as your DHCP server

    environment:
      TZ: ${TZ}
      WEBPASSWORD: ${PIHOLE_PASSWORD}
      FTLCONF_LOCAL_IPV4: ${FTLCONF_LOCAL_IPV4}
      PIHOLE_DNS_: 172.20.0.3#5053
      DHCP_ACTIVE: ${PIHOLE_DHCP_ACTIVE:-false}
      PIHOLE_DOMAIN: ${PIHOLE_DOMAIN:-lan}
      DNSMASQ_LISTENING: 'all'

      TEMPERATUREUNIT: ${PIHOLE_TEMPUNIT:-c}
      WEBTHEME: ${PIHOLE_WEBTHEME:-default-dark}

    labels:
      - diun.enable=true
      - homepage.group=Network
      - homepage.name=Pihole
      - homepage.icon=pihole
      - homepage.href=http://${SERVER_URL}:${PORT_PIHOLE_WEB}/admin
      - homepage.description=Network-wide Adblocking and DNS
      - homepage.widget.type=pihole
      - homepage.widget.url=http://${SERVER_URL}:${PORT_PIHOLE_WEB}
      - homepage.widget.key=${HOMEPAGE_PIHOLE_API}

    volumes:
      - ${CONFIGDIR}/pihole/pihole:/etc/pihole
      - ${CONFIGDIR}/pihole/dnsmasq.d:/etc/dnsmasq.d
      - ${CONFIGDIR}/pihole/resolv.conf:/etc/resolv.conf
    #   https://github.com/pi-hole/docker-pi-hole#note-on-capabilities

    #cap_add:
    #  - NET_ADMIN # Recommended but not required (DHCP needs NET_ADMIN)


    deploy:
      resources:
        limits:
          memory: 256M
    healthcheck:
      test: ["CMD", "dig", "+norecurse", "+retry=0", "@127.0.0.1", "pi.hole"]
    restart: unless-stopped

  speedtest-tracker:
    container_name: speedtest-tracker
    image: ghcr.io/alexjustesen/speedtest-tracker:latest
    profiles:
      - network

    ports:
      - ${PORT_SPEEDTEST}:80

    environment:
      PUID: ${PUID}
      PGID: ${PGID}
      TZ: ${TZ}
    
    labels:
      - diun.enable=true
      - homepage.group=Network
      - homepage.name=Speedtest Tracker
      - homepage.icon=speedtest-tracker
      - homepage.href=http://${SERVER_URL}:${PORT_SPEEDTEST}
      - homepage.description=Build a history of your internet's performance
      - homepage.widget.type=speedtest
      - homepage.widget.url=http://${SERVER_URL}:${PORT_SPEEDTEST}

    volumes:
      - ${CONFIGDIR}/speedtest-tracker:/config

    deploy:
      resources:
        limits:
          memory: 128M
    restart: unless-stopped

  unbound:
    container_name: unbound
    image: mvance/unbound:latest
    profiles:
      - network

    networks:
      dns-net:
        ipv4_address: 172.20.0.3

    labels:
      - diun.enable=true
      - homepage.group=Network
      - homepage.name=Unbound
      - homepage.icon=pihole-unbound
      - homepage.description=A validating, recursive, caching DNS resolver

    volumes:
      - ${CONFIGDIR}/unbound:/opt/unbound/etc/unbound

    deploy:
      resources:
        limits:
          memory: 128M
    healthcheck:
      disable: true
    restart: unless-stopped

####################
# Back End Services
####################

  caddy:
    container_name: caddy
    build:
      context: ${DOCKERDIR}
      dockerfile: caddy.dockerfile #Custom build which uses duckdns
    profiles:
      - external

    depends_on:
      - crowdsec

    external_links:
      - crowdsec:crowdsec
      - jellyfin:jellyfin
      - jellyseerr:jellyseerr
      - tandoor:tandoor
      - uptime_kuma:uptime_kuma

    networks:
      - web
      - caddy-net

    ports:
      - ${PORT_CADDY_HTTP}:80
      - ${PORT_CADDY_HTTPS}:443

    environment:
      LOG_FILE: ${LOGDIR}/caddy/access.log
      DOMAIN: ${DOMAIN}
      EMAIL: ${EMAIL_ADMIN}
      DUCKDNS_API_TOKEN: ${DUCKDNS_TOKEN}
      
      BOUNCER_CADDY_TOKEN: ${BOUNCER_CADDY_TOKEN}

    labels:
      - diun.enable=true
      - homepage.group=Wild West
      - homepage.name=Caddy
      - homepage.icon=caddy
      - homepage.description=Reverse Proxy

    volumes:
      - ${STATICCONFIGDIR}/caddy/Caddyfile:/etc/caddy/Caddyfile:ro # Required. Needs to be an extension-less file NOT a directory
      - ${CONFIGDIR}:/data # Optional, house for certs. Caddy adds its own /caddy/ directory
      - ${CONFIGDIR}:/config # Optional, JSON Config files. Caddy adds its own /caddy/ directory
      - ${STATICDIR}/tandoor_media:/www/tandoor:ro # recipe image files for Tandoor

    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 128M
    healthcheck:
      test: ["CMD", "caddy", "version"]
    restart: unless-stopped
  
  duckdns:
    container_name: duckdns
    image: ghcr.io/linuxserver/duckdns
    profiles:
      - external

    environment:
      PUID: ${PUID}
      PGID: ${PGID}
      TZ: ${TZ}
      SUBDOMAINS: ${DUCKDNS_SUBDOMAINLIST}
      TOKEN: ${DUCKDNS_TOKEN}

    labels:
      - diun.enable=true
      - homepage.group=Wild West
      - homepage.name=DuckDNS
      - homepage.icon=duckdns
      - homepage.description=Dynamic DNS for external access
    volumes:
      - ${CONFIGDIR}/duckdns:/config #optional

    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 32M
    restart: unless-stopped

  diun:
    container_name: diun
    image: ghcr.io/crazy-max/diun:4.22
    profiles:
      - monitor

    depends_on:
      - socky_proxy

    networks:
      - socky_proxy-net
      
    command: serve

    environment:
      TZ: ${TZ}
      LOG_JSON: "true"
      DIUN_NOTIF_DISCORD_WEBHOOKURL: ${DIUN_NOTIF_DISCORD_WEBHOOKURL}
      DIUN_PROVIDERS_DOCKER_ENDPOINT: tcp://socky_proxy:${PORT_SOCKY_PROXY} # redirects socket to the proxy

    labels:
      - diun.enable=true
      - homepage.group=System
      - homepage.name=Diun
      - homepage.icon=/icons/diun.png
      - homepage.description=Docker Image update Notifications
    
    volumes:
      - ${STATICCONFIGDIR}/diun/diun.yml:/diun.yml:ro
      - ${CONFIGDIR}/diun:/data

    healthcheck:
      test: ["CMD", "diun", "--version"]

    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 32M
    restart: unless-stopped

  protonvpn:
    container_name: protonvpn
    image: ghcr.io/linuxserver/wireguard:latest
    profiles:
      - downloads
      - media-request
    
    environment:
      PUID: ${PUID}
      PGID: ${PGID}
      TZ: ${TZ}
    
    networks:
      - peer2peer
      - vpn

    ports:
      - ${PORT_WIREGUARD_OUTBOUND}:51820/udp
      - ${PORT_TORRENT_UI}:${PORT_TORRENT_UI} #default 8080
      - ${PORT_PROWLARR_UI}:9696

    labels:
      - diun.enable=true
      - homepage.group=Back End
      - homepage.name=ProtonVPN
      - homepage.icon=wireguard
      - homepage.description=VPN for Downloads

    volumes:
      - ${CONFIGDIR}/wireguard:/config
      - /lib/modules:/lib/modules:ro

    cap_add:
      - NET_ADMIN
      - SYS_MODULE
    sysctls:
      - net.ipv4.conf.all.src_valid_mark=1

    healthcheck:
      test: ["CMD", "wg", "show"]

    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 64M
    restart: unless-stopped

  socky_proxy:
    container_name: socky_proxy
    image: ghcr.io/tecnativa/docker-socket-proxy:0.1 # A security-enhanced proxy for the Docker Socket.
    profiles:
      - admin
      - external
      - monitor

    privileged: true
  
    networks:
      - socky_proxy-net

    ports:
      - ${PORT_SOCKY_PROXY}:2375

    environment:
      # Variables match the URL prefix (i.e. AUTH blocks access to /auth/* parts of the API, etc).
      #   0 - revoke access
      #   1 - grant access
      CONTAINERS: 1 # crowdsec, diun, homepage
      IMAGES: 1 # diun
      INFO: 1 # crowdsec

    labels:
      - diun.enable=true
      - homepage.group=Back End
      - homepage.name=Docker Socket Proxy

    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
    
    healthcheck:
      test: wget --spider http://${SERVER_URL}:${PORT_SOCKY_PROXY}/version || exit 1
      interval: "30s"
      timeout: "5s"
      retries: 3
      start_period: "30s"

    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 32M

    restart: always

####################
# System Information
####################

  dozzle:
    container_name: dozzle
    image: amir20/dozzle:latest
    profiles:
      - monitor

    depends_on:
      - socky_proxy

    networks:
      - socky_proxy-net

    ports:
      - ${PORT_DOZZLE}:8080

    environment:
      DOCKER_HOST: tcp://socky_proxy:${PORT_SOCKY_PROXY} # redirects socket to the proxy
    
    labels:
      - diun.enable=true
      - homepage.group=System
      - homepage.name=Dozzle
      - homepage.icon=dozzle
      - homepage.href=http://${SERVER_URL}:${PORT_DOZZLE}
      - homepage.description=Docker Logs


    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 128M
    restart: always

  scrutiny:
    container_name: scrutiny
    image: ghcr.io/analogj/scrutiny:master-omnibus
    profiles:
      - monitor

    ports:
      - ${PORT_SCRUTINY}:8080 # webapp
      - ${PORT_SCRUTINY_DB}:8086 # influxDB admin

    labels:
      - diun.enable=true
      - homepage.group=System
      - homepage.name=Scrutiny
      - homepage.icon=scrutiny
      - homepage.href=http://${SERVER_URL}:${PORT_SCRUTINY}
      - homepage.description=Harddrive Health Monitoring
      - homepage.widget.type=scrutiny
      - homepage.widget.url=http://${SERVER_URL}:${PORT_SCRUTINY}

    devices:
      - /dev/sda
      - /dev/sdb
    
    volumes:
      - /run/udev:/run/udev:ro
      - ${CONFIGDIR}/scrutiny:/opt/scrutiny/config
      - ${DBDIR}/scrutiny:/opt/scrutiny/influxdb

    cap_add:
      - SYS_RAWIO #necessary to allow smartctl permission to query your device SMART data 
      - SYS_ADMIN #necessary for NVMe drives

    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 256M
    restart: unless-stopped


####################
# Security
####################

  crowdsec:
    container_name: crowdsec
    image: crowdsecurity/crowdsec:v1.4.6
    profiles:
      - external

    depends_on:
      - socky_proxy
    
    networks:
      - caddy-net # provides access for the reverse proxy
      - socky_proxy-net

    ports:
      - ${PORT_CROWDSEC_API}:8080 # exposes a REST API for bouncers, cscli and communication between crowdsec agent and local api
      - ${PORT_CROWDSEC_METRICS}:6060 #exposes prometheus metrics on /metrics and pprof debugging metrics on /debug

    environment:
      TZ: ${TZ}
      GID: ${PGID}
      COLLECTIONS: "crowdsecurity/linux crowdsecurity/caddy crowdsecurity/linux-lpe timokoessler/uptime-kuma LePresidente/jellyseerr crowdsecurity/pgsql crowdsecurity/endlessh schiz0phr3ne/sonarr schiz0phr3ne/radarr schiz0phr3ne/prowlarr"
      DOCKER_HOST: tcp://socky_proxy:${PORT_SOCKY_PROXY} # redirects socket to the proxy. Needed for the config.yaml direction to scan other docker containers

    volumes:
      - ${STATICCONFIGDIR}/crowdsec/acquis.yaml:/etc/crowdsec/acquis.yaml:ro
      - crowdsec-config:/etc/crowdsec/
      - crowdsec-db:/var/lib/crowdsec/data/
    #  - ${CONFIGDIR}/crowdsec:/etc/crowdsec/ # This is my preferred way to get things working, but currently doesn't work

      ## Log Files
      - /var/log/auth.log:/logs/auth.log:ro
      - /var/log/syslog.log:/logs/syslog.log:ro
      
    labels:
      - diun.enable=true
      - homepage.group=Wild West
      - homepage.name=Crowdsec
      - homepage.icon=crowdsec
      - homepage.href=https://app.crowdsec.net/login?next=/instances
      - homepage.description=Crowd-sourced Intrusion Prevention


    healthcheck:
      test: ["CMD", "cscli", "version"]

    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 128M
    restart: always

  endlessh:
    image: ghcr.io/linuxserver/endlessh:latest
    container_name: endlessh
    profiles:
      - external

    ports:
      - ${PORT_SSHPROTECT:-22}:2222
    
    environment:
      PUID: ${PUID}
      PGID: ${PGID}
      TZ: ${TZ}
      MSDELAY: ${ESSH_MSDELAY:-10000} #optional
      MAXLINES: ${ESSH_MAXLINES:-32} #optional
      MAXCLIENTS: ${ESSH_MAXCLIENTS:-4096} #optional
      LOGFILE: ${DEBUG_LOG:-false} #optional. By default, the app logs to container log. If this is set to true, the log will be output to file under /config/logs/endlessh (/config needs to be mapped).
      BINDFAMILY: ${ESSH_BINDFAMILY} #optional

    #volumes:
    #  - ${CONFIGDIR}/endlessh:/config #Required if LOGFILE is set to true.

    labels:
      - diun.enable=true
      - homepage.group=Wild West
      - homepage.name=EndlessH
      - homepage.icon=endlessh
      - homepage.description=SSH Tarpit

    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 32M  
    restart: unless-stopped


####################
# Docker Environment
####################

volumes:
  crowdsec-config:
    external: true
  crowdsec-db:
    external: true

networks:
  web:
    name: web
    external: true
  caddy-net:
    name: caddy-net
    external: true
    
  htpc-net:
    name: htpc-net
  tandoor-net:
    name: tandoor-net
  rallly-net:
    name: rallly-net

  peer2peer:
    name: peer2peer
  vpn:
    name: vpn
    internal: true
  dns-net:
    name: dns-net
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16

  socky_proxy-net:
    name: socky_proxy-net
