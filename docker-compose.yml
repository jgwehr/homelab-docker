version: "3.9"

services:

####################
# Admin
####################
  
  homepage:
    image: ghcr.io/benphelps/homepage:latest
    container_name: homepage
    profiles:
      - admin

    networks:
      - socky_proxy-net

    ports:
      - ${PORT_DASH_HTTP}:3000

    environment:
      PUID: ${PUID}
      PGID: ${PGID}
      TZ: ${TZ}

    volumes:
      - ${CONFIGDIR}/homepage:/app/config # Make sure your local config directory exists
      - ${STATICDIR}/icons:/app/public/icons #Optional. for custom icons
      - ${STATICDIR}/wallpaper:/app/public/images #Optional. for custom backgrounds

      - ${SYS_DISK1}:/dev/sda1:ro #harddrive space monitoring
      - ${SYS_DISK2}:/dev/sdb1:ro #harddrive space monitoring
      - ${SYS_DISK3}:/dev/sdc1:ro #harddrive space monitoring
    
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 128M
    restart: unless-stopped

  uptime-kuma:
    container_name: uptime_kuma
    image: louislam/uptime-kuma
    profiles:
      - admin
    
    networks:
      - caddy-net # provides access for the reverse proxy
    
    ports:
      - ${PORT_UPKUMA}:3001
    
    labels:
      - diun.enable=true
      - homepage.group=System
      - homepage.name=Uptime Kuma
      - homepage.icon=uptime-kuma
      - homepage.href=http://${SERVER_URL}:${PORT_UPKUMA}
      - homepage.widget.type=uptimekuma
      - homepage.widget.url=http://${SERVER_URL}:${PORT_UPKUMA}
      - homepage.widget.slug=default

    volumes:
      - ${CONFIGDIR}/uptime-kuma:/app/data
    
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 128M
    restart: always

####################
# Recipes
####################

  tandoor:
    container_name: tandoor
    image: vabene1111/recipes:latest
    profiles:
      - recipes

    depends_on:
      tandoor_db:
        condition: service_healthy
    
    networks:
      - caddy-net # provides access for the reverse proxy
      - tandoor-net

    ports:
      - ${PORT_TANDOOR}:8080

    env_file:
      - ${STATICCONFIGDIR}/tandoor/tandoor.env

    environment:
      TIMEZONE: ${TZ}

    labels:
      - diun.enable=true
      - homepage.group=Lifestyle
      - homepage.name=Tandoor
      - homepage.icon=tandoorrecipes
      - homepage.href=http://${EXTERNAL_TANDOOR}.${DOMAIN}
      - homepage.description=Recipes and Meal Planning

    volumes:
      - ${STATICDIR}/tandoor_media:/opt/recipes/mediafiles

    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 512M
    restart: unless-stopped

  tandoor_db:
    container_name: tandoor_db
    image: postgres:alpine

    profiles:
      - recipes

    networks:
      - tandoor-net

    env_file:
      - ${STATICCONFIGDIR}/tandoor/tandoor.env

    labels:
      - homepage.group=Back End
      - homepage.name=Tandoor | Postgres DB
      - homepage.icon=postgres

    volumes:
      - ${DBDIR}/tandoor_db:/var/lib/postgresql/data

    healthcheck:
      interval: 60s
      retries: 10
      test:
        - CMD
        - pg_isready
        - '-q'
        - '-d'
        - ${TANDOOR_DB_NAME}
        - '-U'
        - ${TANDOOR_DB_USER}
      timeout: 45s
      
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 128M
    restart: unless-stopped

####################
# Media
####################

  jellyfin:
    container_name: jellyfin
    image: jellyfin/jellyfin
    profiles:
      - media
      - media-request
      
    user: ${PUID}:${PGID}
    group_add:
      - ${GID_HARDWAREACC} # gid of your `render` group

    networks:
      - htpc-net
      - caddy-net # provides access for the reverse proxy
    
    ports:
      - ${PORT_HTPC_HTTP}:8096 #HTTP webUI
      - ${PORT_HTPC_HTTPS}:8920 #HTTPS webUI
      - ${PORT_HTPC_LOCAL}:7359/udp #optional. Allows clients to discover Jellyfin on the local network.
      - ${PORT_HTPC_DLNA}:1900/udp #optional. Service discovery used by DNLA and clients.

    environment:
      TZ: ${TZ}
      JELLYFIN_PublishedServerUrl: ${SERVER_URL}:${PORT_HTPC_HTTP} #optional. The Server URL to publish in udp Auto Discovery response.

    labels:
      - diun.enable=true
      - homepage.group=Media
      - homepage.name=Jellyfin
      - homepage.icon=jellyfin
      - homepage.href=http://${SERVER_URL}:${PORT_HTPC_HTTP}
      - homepage.widget.type=jellyfin
      - homepage.widget.url=http://${SERVER_URL}:${PORT_HTPC_HTTP}
      - homepage.widget.key=${HOMEPAGE_JELLYFIN_API}

    devices:
      - /dev/dri/renderD128:/dev/dri/renderD128 # VAAPI
      - /dev/dri/card0:/dev/dri/card0

    volumes:
      - ${CONFIGDIR}/jellyfin:/config
      - ${CACHEDIR}/jellyfin:/cache
      - ${MEDIADIR}:/media:ro #naming, etc all handled by *arr apps
    
    #deploy:
    #  resources:
    #    limits:
    #      cpus: '2'
    #      memory: 1024M
    restart: unless-stopped
  
  jellyseerr:
    container_name: jellyseerr
    image: fallenbagel/jellyseerr:latest
    profiles:
      - media-request

    depends_on:
      - sonarr
      - radarr
      - jellyfin # for authentication

    networks:
      - htpc-net
      - caddy-net # provides access for the reverse proxy

    ports:
      - ${PORT_HTPC_REQUESTS}:5055

    environment:
      LOG_LEVEL: ${LOG_LEVEL}
      TZ: ${TZ}

    labels:
      - diun.enable=true
      - homepage.group=Media
      - homepage.name=Request Movies and TV Shows
      - homepage.icon=jellyseerr
      - homepage.href=http://${SERVER_URL}:${PORT_HTPC_REQUESTS}
      - homepage.widget.type=jellyseerr
      - homepage.widget.url=http://${SERVER_URL}:${PORT_HTPC_REQUESTS}
      - homepage.widget.key=${HOMEPAGE_JELLYSEERR_API}

    volumes:
        - ${CONFIGDIR}/jellyseerr:/app/config
    
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 512M
    restart: unless-stopped
  
  qbittorrent:
    container_name: qbittorrent
    image: ghcr.io/linuxserver/qbittorrent
    profiles:
      - downloads
      - media-request

    depends_on:
      gluetun:
        condition: service_healthy

    network_mode: service:gluetun # ONLY provide network config in the gluetun container

    environment:
        PUID: ${PUID}
        PGID: ${PGID}
        TZ: ${TZ}
        UMASK_SET: 022
        WEBUI_PORT: 8080

    labels:
      - diun.enable=true
      - homepage.group=Downloads
      - homepage.name=Torrents
      - homepage.icon=qbittorrent
      - homepage.href=http://${SERVER_URL}:${PORT_TORRENT_UI}
      - homepage.widget.type=qbittorrent
      - homepage.widget.url=http://${SERVER_URL}:${PORT_TORRENT_UI}
      - homepage.widget.username=${HOMEPAGE_QBITTORENT_USERNAME}
      - homepage.widget.password=${HOMEPAGE_QBITTORENT_PASSWORD}

    volumes:
        - ${CONFIGDIR}/qbt:/config
        - ${DOWNLOADDIR}:/data/downloads
    
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 512M
    restart: unless-stopped

  prowlarr:
    container_name: prowlarr
    image: lscr.io/linuxserver/prowlarr:develop
    profiles:
      - media-request

    depends_on:
      gluetun:
        condition: service_healthy
      qbittorrent:
        condition: service_started

    network_mode: service:gluetun # ONLY provide network config in the gluetun container

    environment:
      PUID: ${PUID}
      PGID: ${PGID}
      TZ: ${TZ}

    labels:
      - diun.enable=true
      - homepage.group=Downloads
      - homepage.name=Indexers
      - homepage.icon=prowlarr
      - homepage.href=http://${SERVER_URL}:${PORT_PROWLARR_UI}
      - homepage.widget.type=prowlarr
      - homepage.widget.url=http://${SERVER_URL}:${PORT_PROWLARR_UI}
      - homepage.widget.key=${HOMEPAGE_PROWLARR_API}
      
    volumes:
      - ${CONFIGDIR}/prowlarr:/config

    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 600M
    restart: unless-stopped
  
  radarr:
    container_name: radarr
    image: ghcr.io/linuxserver/radarr:latest
    profiles:
      - media-request
    
    depends_on:
      prowlarr:
        condition: service_started
      gluetun:
        condition: service_healthy

    network_mode: service:gluetun # ONLY provide network config in the gluetun container

    environment:
      PUID: ${PUID}
      PGID: ${PGID}
      TZ: ${TZ}

    labels:
      - diun.enable=true
      - homepage.group=Downloads
      - homepage.name=Movies
      - homepage.icon=radarr
      - homepage.href=http://${SERVER_URL}:${PORT_RADARR_UI}
      - homepage.widget.type=radarr
      - homepage.widget.url=http://${SERVER_URL}:${PORT_RADARR_UI}
      - homepage.widget.key=${HOMEPAGE_RADARR_API}

    volumes:
        - /etc/localtime:/etc/localtime:ro
        - ${CONFIGDIR}/radarr3:/config
        - ${DATADIR}:/data
    
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 512M
    restart: unless-stopped

  sonarr:
    container_name: sonarr
    image: ghcr.io/linuxserver/sonarr:latest
    profiles:
      - media-request

    depends_on:
      prowlarr:
        condition: service_started
      gluetun:
        condition: service_healthy

    network_mode: service:gluetun # ONLY provide network config in the gluetun container

    environment:
      PUID: ${PUID}
      PGID: ${PGID}
      TZ: ${TZ}

    labels:
      - diun.enable=true
      - homepage.group=Downloads
      - homepage.name=TV Shows
      - homepage.icon=sonarr
      - homepage.href=http://${SERVER_URL}:${PORT_SONARR_UI}
      - homepage.widget.type=sonarr
      - homepage.widget.url=http://${SERVER_URL}:${PORT_SONARR_UI}
      - homepage.widget.key=${HOMEPAGE_SONARR_API}

    volumes:
        - /etc/localtime:/etc/localtime:ro
        - ${CONFIGDIR}/sonarr:/config
        - ${DATADIR}:/data
    
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 512M
    restart: unless-stopped

  podgrab:
    container_name: podgrab
    image: ghcr.io/akhilrex/podgrab
    profiles:
      - media-request
      - downloads

    ports:
      - ${PORT_PODGRAB}:8080

    environment:
      - CHECK_FREQUENCY=1800

    labels:
      - homepage.group=Media
      - homepage.name=Podgrab
      - homepage.href=http://${SERVER_URL}:${PORT_PODGRAB}
      - homepage.description=Podcast manager/downloader/archiver tool to download podcast episodes

    volumes:
        - /etc/localtime:/etc/localtime:ro
        - ${CONFIGDIR}/podgrab:/config
        - ${MEDIADIR}/podcasts:/assets

    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 512M
    restart: unless-stopped

####################
# Automatic Ripping Machine
####################

  automatic-ripping-machine:
    container_name: automatic-ripping-machine
    image: 1337server/automatic-ripping-machine:latest
    profiles:
      - ripping

    privileged: true

    ports:
      - ${PORT_RIPPING}:8080

    environment:
      ARM_UID: ${RIPPING_UID}
      ARM_GID: ${RIPPING_GID}

    labels:
      - homepage.group=Downloads
      - homepage.name=Automatic Ripping Machine
      - homepage.href=http://${SERVER_URL}:${PORT_RIPPING}
      - homepage.description=Disk-in Music/Movie Ripping

    devices:
      - /dev/sr0:/dev/sr0
      
    volumes:
      - ${CONFIGDIR}/ripping:/etc/arm/config
      - /etc/localtime:/etc/localtime:ro

      - ${RIPPING_HOME}:/home/arm
      - ${LOGDIR}/ripping:/home/arm/logs

      - ${DOWNLOADDIR}/rips/video:/home/arm/media/completed
      - ${DOWNLOADDIR}/rips/music:/home/arm/Music


    deploy:
      resources:
        limits:
          cpus: '5'
          memory: 2048M #for a single rip, ARM is not RAM limited.
    restart: 'no'

####################
# Gloomhaven
####################

  gloomhaven:
    container_name: gloomhaven
    image: ghcr.io/lurkars/ghs
    profiles:
      - gloomhaven

    networks:
      - caddy-net

    ports:
      - ${PORT_GHS_CLIENT}:80

    labels:
      - diun.enable=true
      - homepage.group=Games
      - homepage.name=Gloomhaven Secretariat
      - homepage.icon=/icons/ghs.png
      - homepage.href=http://${EXTERNAL_GHS}.${DOMAIN}
      

    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 128M
    restart: unless-stopped

  gloomhaven-server:
    container_name: gloomhaven-server
    image: ghcr.io/lurkars/ghs-server
    profiles:
      - gloomhaven

    networks:
      - caddy-net
      
    ports:
      - ${PORT_GHS_SERVER}:8080

    labels:
      - diun.enable=true
      - homepage.group=Back End
      - homepage.name=Gloomhaven Secretariat Server
      - homepage.icon=/icons/ghs.png
      - homepage.href=http://${SERVER_URL}:${PORT_GHS_SERVER}

    volumes:
      - ${CONFIGDIR}/ghs:/root/.ghs # optional, to back up Party Sheets etc.

    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
    restart: unless-stopped

####################
# Lifestyle
####################

  changedetection:
    container_name: changedetection
    image: ghcr.io/dgtlmoon/changedetection.io

    profiles:
      - lifestyle

    depends_on:
      - chrome
    
    ports:
      - ${PORT_CHANGEDETECTION}:5000

    environment:
      PUID: ${PUID}
      PGID: ${PGID}
      TZ: ${TZ}
      WEBDRIVER_URL: http://chrome:${PORT_WEBDRIVER}/wd/hub

    labels:
      - diun.enable=true
      - homepage.group=Lifestyle
      - homepage.name=Change Detection
      - homepage.icon=changedetection-io
      - homepage.href=http://${SERVER_URL}:${PORT_CHANGEDETECTION}
      - homepage.widget.type=changedetectionio
      - homepage.widget.url=http://${SERVER_URL}:${PORT_CHANGEDETECTION}
      - homepage.widget.key=${HOMEPAGE_CHANGEDETECTION_API}

    volumes:
      - ${CONFIGDIR}/changedetection:/datastore
    
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 128M
    restart: unless-stopped
  
  pinry:
    container_name: pinry
    image: getpinry/pinry

    profiles:
      - lifestyle

    networks:
      - caddy-net

    ports:
      - ${PORT_PINRY}:80

    environment:
      ALLOW_NEW_REGISTRATIONS: 'True'
      SECRET_KEY: ${PINRY_SECRETKEY}
    
    labels:
      - diun.enable=true
      - homepage.group=Lifestyle
      - homepage.name=Pinry
      - homepage.icon=pinry
      - homepage.href=http://${EXTERNAL_PINRY}.${DOMAIN}
      - homepage.description=Save, tag, and share images, videos and webpages

    volumes:
      - ${CONFIGDIR}/pinry:/data
    
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
    restart: unless-stopped

####################
# Rallly
####################

  rallly:
    container_name: rallly
    image: lukevella/rallly:latest

    profiles:
      - calendar

    networks:
      - caddy-net
      - rallly-net

    depends_on:
      rallly_db:
        condition: service_healthy

    ports:
      - ${PORT_RALLLY}:3000

    environment:
      DATABASE_URL: postgres://${RALLLY_DB_USER}:${RALLLY_DB_PASSWORD}@rallly_db:5432/${RALLLY_DB_NAME}
      SECRET_PASSWORD: ${RALLLY_SECRETKEY}
      AUTH_REQUIRED: true
      DISABLE_LANDING_PAGE: true
      SUPPORT_EMAIL: ${RALLLY_SUPPORTEMAIL}
      NEXT_PUBLIC_BASE_URL: https://${EXTERNAL_RALLLY}.${DOMAIN} # defined in the Caddyfile

      SMTP_HOST: ${RALLLY_SMTP_HOST}
      SMTP_PORT:  ${RALLLY_SMTP_PORT}
      SMTP_SECURE: ${RALLLY_SMTP_SECURE}
      SMTP_USER: ${RALLLY_SMTP_USER}
      SMTP_PWD: ${RALLLY_SMTP_PASSWORD}
    
    labels:
      - diun.enable=true
      - homepage.group=Lifestyle
      - homepage.name=Rallly
      - homepage.icon=/icons/rallly.png
      - homepage.href=http://${EXTERNAL_RALLLY}.${DOMAIN}
      - homepage.description=Schedule hangouts with friends

    volumes:
      - ${CONFIGDIR}/rallly:/data
    
    deploy:
      resources:
        limits:
          cpus: '0.7'
          memory: 256M
    restart: unless-stopped

  rallly_db:
    container_name: rallly_db
    image: postgres:alpine

    profiles:
      - calendar

    networks:
      - rallly-net

    ports:
      - ${PORT_RALLLY_DB}:5432

    environment:
      POSTGRES_DB: ${RALLLY_DB_NAME}
      POSTGRES_USER: ${RALLLY_DB_USER}
      POSTGRES_PASSWORD: ${RALLLY_DB_PASSWORD}

    labels:
      - homepage.group=Back End
      - homepage.name=Rallly | Postgres DB
      - homepage.icon=postgres

    volumes:
      - ${DBDIR}/rallly:/var/lib/postgresql/data

    healthcheck:
      interval: 60s
      retries: 10
      test:
        - CMD
        - pg_isready
        - '-q'
        - '-d'
        - ${RALLLY_DB_NAME}
        - '-U'
        - ${RALLLY_DB_USER}
      timeout: 45s
      
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 128M
    restart: unless-stopped

####################
# Paperless
####################

  paperless:
    container_name: paperless
    image: ghcr.io/paperless-ngx/paperless-ngx:latest

    profiles:
      - paperless

    depends_on:
      paperless_db:
        condition: service_healthy
      paperless_broker:
        condition: service_healthy

    networks:
      - paperless-net
    
    ports:
      - ${PORT_PAPERLESS}:8000

    environment:
      USERMAP_UID: ${PUID}
      USERMAP_GID: ${PGID}
      PAPERLESS_TIME_ZONE: ${TZ}
      PAPERLESS_OCR_LANGUAGE: 'eng'
      PAPERLESS_SECRET_KEY: ${PAPERLESS_SECRETKEY}
      PAPERLESS_DATE_ORDER: MDY

      PAPERLESS_WEBSERVER_WORKERS: 2
      PAPERLESS_CONSUMER_POLLING: 600 # only needed if not NFS: https://docs.paperless-ngx.com/troubleshooting/#consumer-fails-to-pickup-any-new-files

      PAPERLESS_REDIS: redis://paperless_broker:6379
      PAPERLESS_DBHOST: paperless_db
      PAPERLESS_DBNAME: ${PAPERLESS_DB_NAME}
      PAPERLESS_DBUSER: ${PAPERLESS_DB_USER}
      PAPERLESS_DBPASS: ${PAPERLESS_DB_PASSWORD}


    labels:
      - diun.enable=true
      - homepage.group=Office
      - homepage.name=Paperless
      - homepage.icon=paperless
      - homepage.href=http://${SERVER_URL}:${PORT_PAPERLESS}
      - homepage.description=Scan, index and archive all your physical documents
      - homepage.widget.type=paperlessngx
      - homepage.widget.url=http://${SERVER_URL}:${PORT_PAPERLESS}
      - homepage.widget.username=${HOMEPAGE_PAPERLESS_USERNAME}
      - homepage.widget.password=${HOMEPAGE_PAPERLESS_PASSWORD}

    volumes:
      - ${CONFIGDIR}/paperless:/usr/src/paperless/data
      - ${STATICDIR}/paperless:/usr/src/paperless/media
      - ${BACKUPDIR}/paperless:/usr/src/paperless/export # used for backup/exporting documents via paperless document_exporter
      - ${PAPERLESS_CONSUMEDIR}:/usr/src/paperless/consume

    healthcheck:
      test: ["CMD", "curl", "-fs", "-S", "--max-time", "2", "http://localhost:8000"]
      interval: 60s
      timeout: 10s
      retries: 5
    
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4096M
    restart: unless-stopped


  paperless_broker:
    container_name: paperless_broker
    image: redis:7

    profiles:
      - paperless

    networks:
      - paperless-net

    labels:
      - homepage.group=Back End
      - homepage.name=Paperless | Redis
      - homepage.icon=redis

    volumes:
      - ${DBDIR}/paperless-redis:/data

    healthcheck:
      interval: 60s
      retries: 5
      test: [ "CMD", "redis-cli", "--raw", "incr", "ping" ]
      timeout: 45s

    deploy:
        resources:
          limits:
            cpus: '1'
            memory: 256M
    restart: unless-stopped

  paperless_db:
    container_name: paperless_db
    image: postgres:alpine

    profiles:
      - paperless

    networks:
      - paperless-net

    environment:
      POSTGRES_DB: ${PAPERLESS_DB_NAME}
      POSTGRES_USER: ${PAPERLESS_DB_USER}
      POSTGRES_PASSWORD: ${PAPERLESS_DB_PASSWORD}

    labels:
      - homepage.group=Back End
      - homepage.name=Paperless | Postgres DB
      - homepage.icon=postgres

    volumes:
      - ${DBDIR}/paperless-postgres:/var/lib/postgresql/data

    healthcheck:
      interval: 60s
      retries: 10
      test:
        - CMD
        - pg_isready
        - '-q'
        - '-d'
        - ${PAPERLESS_DB_NAME}
        - '-U'
        - ${PAPERLESS_DB_USER}
      timeout: 45s
      
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 256M
    restart: unless-stopped

####################
# Network
####################

  pihole:
  # More info at https://github.com/pi-hole/docker-pi-hole/ and https://docs.pi-hole.net/
    container_name: pihole
    hostname: pihole
    image: pihole/pihole:latest
    profiles:
      - network
    
    depends_on:
      - unbound

    networks:
      dns-net:
        ipv4_address: 172.20.0.2

    # For DHCP it is recommended to remove these ports and instead add: network_mode: "host"
    ports:
      - ${PORT_PIHOLE_WEB:-80}:80
      - ${PORT_PIHOLE_WEBSSL:-443}:443
      - ${PORT_PIHOLE_TCP:-53}:53/tcp
      - ${PORT_PIHOLE_UDP:-53}:53/udp
    #  - ${PORT_PIHOLE_DHCP:-67}:67/udp # Only required if you are using Pi-hole as your DHCP server

    environment:
      TZ: ${TZ}
      WEBPASSWORD: ${PIHOLE_PASSWORD}
      FTLCONF_LOCAL_IPV4: ${FTLCONF_LOCAL_IPV4}
      PIHOLE_DNS_: 172.20.0.3#5053
      DHCP_ACTIVE: ${PIHOLE_DHCP_ACTIVE:-false}
      PIHOLE_DOMAIN: ${PIHOLE_DOMAIN:-lan}
      DNSMASQ_LISTENING: 'all'

      TEMPERATUREUNIT: ${PIHOLE_TEMPUNIT:-c}
      WEBTHEME: ${PIHOLE_WEBTHEME:-default-dark}

    labels:
      - diun.enable=true
      - homepage.group=Network
      - homepage.name=Pihole
      - homepage.icon=pi-hole
      - homepage.href=http://${SERVER_URL}:${PORT_PIHOLE_WEB}/admin
      - homepage.widget.type=pihole
      - homepage.widget.url=http://${SERVER_URL}:${PORT_PIHOLE_WEB}
      - homepage.widget.key=${HOMEPAGE_PIHOLE_API}

    volumes:
      - ${CONFIGDIR}/pihole/pihole:/etc/pihole
      - ${CONFIGDIR}/pihole/dnsmasq.d:/etc/dnsmasq.d
      - ${CONFIGDIR}/pihole/resolv.conf:/etc/resolv.conf
    #   https://github.com/pi-hole/docker-pi-hole#note-on-capabilities

    #cap_add:
    #  - NET_ADMIN # Recommended but not required (DHCP needs NET_ADMIN)


    deploy:
      resources:
        limits:
          memory: 256M
    healthcheck:
      test: ["CMD", "dig", "+norecurse", "+retry=0", "@127.0.0.1", "pi.hole"]
    restart: unless-stopped

  speedtest-tracker:
    container_name: speedtest-tracker
    image: ghcr.io/alexjustesen/speedtest-tracker:latest
    profiles:
      - network

    ports:
      - ${PORT_SPEEDTEST}:80

    environment:
      PUID: ${PUID}
      PGID: ${PGID}
      TZ: ${TZ}
    
    labels:
      - diun.enable=true
      - homepage.group=Network
      - homepage.name=Speedtest Tracker
      - homepage.icon=speedtest-tracker
      - homepage.href=http://${SERVER_URL}:${PORT_SPEEDTEST}
      - homepage.widget.type=speedtest
      - homepage.widget.url=http://${SERVER_URL}:${PORT_SPEEDTEST}

    volumes:
      - ${CONFIGDIR}/speedtest-tracker:/config

    deploy:
      resources:
        limits:
          memory: 128M
    restart: unless-stopped

  unbound:
    container_name: unbound
    image: mvance/unbound:latest
    profiles:
      - network

    networks:
      dns-net:
        ipv4_address: 172.20.0.3

    labels:
      - diun.enable=true
      - homepage.group=Network
      - homepage.name=Unbound
      - homepage.icon=pi-hole-unbound

    volumes:
      - ${CONFIGDIR}/unbound:/opt/unbound/etc/unbound

    deploy:
      resources:
        limits:
          memory: 128M
    healthcheck:
      disable: true
    restart: unless-stopped

####################
# Back End Services
####################

  caddy:
    container_name: caddy
    build:
      context: ${DOCKERDIR}
      dockerfile: caddy.dockerfile #Custom build which uses duckdns
    profiles:
      - external

    depends_on:
      - crowdsec

    external_links:
      - crowdsec:crowdsec

    networks:
      - web
      - caddy-net

    ports:
      - ${PORT_CADDY_HTTP}:80
      - ${PORT_CADDY_HTTPS}:443

    environment:
      LOG_FILE: ${LOGDIR}/caddy/access.log
      DOMAIN: ${DOMAIN}
      EMAIL: ${EMAIL_ADMIN}
      DUCKDNS_API_TOKEN: ${DUCKDNS_TOKEN}
      
      BOUNCER_CADDY_TOKEN: ${BOUNCER_CADDY_TOKEN}

    labels:
      - diun.enable=true
      - homepage.group=Wild West
      - homepage.name=Caddy
      - homepage.icon=caddy
      - homepage.description=Reverse Proxy

    volumes:
      - ${STATICCONFIGDIR}/caddy/Caddyfile:/etc/caddy/Caddyfile:ro # Required. Needs to be an extension-less file NOT a directory
      - ${CONFIGDIR}:/data # Optional, house for certs. Caddy adds its own /caddy/ directory
      - ${CONFIGDIR}:/config # Optional, JSON Config files. Caddy adds its own /caddy/ directory
      - ${STATICDIR}/tandoor_media:/www/tandoor:ro # recipe image files for Tandoor

    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 128M
    healthcheck:
      test: ["CMD", "caddy", "version"]
    restart: unless-stopped
  
  duckdns:
    container_name: duckdns
    image: ghcr.io/linuxserver/duckdns
    profiles:
      - external

    environment:
      PUID: ${PUID}
      PGID: ${PGID}
      TZ: ${TZ}
      SUBDOMAINS: ${DUCKDNS_SUBDOMAINLIST}
      TOKEN: ${DUCKDNS_TOKEN}

    labels:
      - diun.enable=true
      - homepage.group=Wild West
      - homepage.name=DuckDNS
      - homepage.icon=duckdns
      - homepage.description=Dynamic DNS for external access
    volumes:
      - ${CONFIGDIR}/duckdns:/config #optional

    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 32M
    restart: unless-stopped

  diun:
    container_name: diun
    image: ghcr.io/crazy-max/diun:4.22
    profiles:
      - monitor

    depends_on:
      - socky_proxy

    networks:
      - socky_proxy-net
      
    command: serve

    environment:
      TZ: ${TZ}
      LOG_JSON: "true"
      DIUN_NOTIF_DISCORD_WEBHOOKURL: ${DIUN_NOTIF_DISCORD_WEBHOOKURL}
      DIUN_PROVIDERS_DOCKER_ENDPOINT: tcp://socky_proxy:${PORT_SOCKY_PROXY} # redirects socket to the proxy

    labels:
      - diun.enable=true
      - homepage.group=System
      - homepage.name=Diun
      - homepage.icon=/icons/diun.png
      - homepage.description=Docker Image update Notifications
    
    volumes:
      - ${STATICCONFIGDIR}/diun/diun.yml:/diun.yml:ro
      - ${CONFIGDIR}/diun:/data

    healthcheck:
      test: ["CMD", "diun", "--version"]

    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 32M
    restart: unless-stopped

  chrome:
    container_name: chrome
    image: selenium/standalone-chrome:4.8.3

    profiles:
      - lifestyle

    ports:
      - ${PORT_WEBDRIVER}:4444

    shm_size: 2gb

    labels:
      - diun.enable=true
      - homepage.group=Back End
      - homepage.name=Chrome
      - homepage.icon=chrome

    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
    restart: unless-stopped
  
  gluetun:
    container_name: gluetun
    image: ghcr.io/qdm12/gluetun
    # line above must be uncommented to allow external containers to connect. See https://github.com/qdm12/gluetun/wiki/Connect-a-container-to-gluetun#external-container-to-gluetun
    profiles:
      - downloads
      - media-request
    
    environment:
      PUID: ${PUID}
      PGID: ${PGID}
      TZ: ${TZ}

      VPN_SERVICE_PROVIDER: ${VPN_SERVICE_PROVIDER}
      OPENVPN_USER: ${VPN_USER}
      OPENVPN_PASSWORD: ${VPN_PASSWORD}

      SERVER_HOSTNAMES: ${VPN_HOSTNAMES}

    ports:
      - ${PORT_GLUETUN_CONTROL}:8000
      - ${PORT_TORRENT_UI}:8080
      - ${PORT_PROWLARR_UI}:9696
      - ${PORT_SONARR_UI}:8989
      - ${PORT_RADARR_UI}:7878

    labels:
      - homepage.group=Back End
      - homepage.name=VPN for Downloads
      - homepage.icon=gluetun
      - homepage.weight=100
      - homepage.widget.type=gluetun
      - homepage.widget.url=http://${SERVER_URL}:${PORT_GLUETUN_CONTROL}

    devices:
      - /dev/net/tun:/dev/net/tun

    volumes:
      - ${CONFIGDIR}/gluetun:/gluetun

    cap_add:
      - NET_ADMIN
    
    restart: unless-stopped


  socky_proxy:
    container_name: socky_proxy
    image: ghcr.io/tecnativa/docker-socket-proxy:0.1 # A security-enhanced proxy for the Docker Socket.
    profiles:
      - admin
      - external
      - monitor

    privileged: true
  
    networks:
      - socky_proxy-net

    ports:
      - ${PORT_SOCKY_PROXY}:2375

    environment:
      # Variables match the URL prefix (i.e. AUTH blocks access to /auth/* parts of the API, etc).
      #   0 - revoke access
      #   1 - grant access
      CONTAINERS: 1 # crowdsec, diun, homepage
      IMAGES: 1 # diun
      INFO: 1 # crowdsec

    labels:
      - diun.enable=true
      - homepage.group=Back End
      - homepage.name=Docker Socket Proxy

    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
    
    healthcheck:
      test: wget --spider http://${SERVER_URL}:${PORT_SOCKY_PROXY}/version || exit 1
      interval: "30s"
      timeout: "5s"
      retries: 3
      start_period: "30s"

    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 32M

    restart: always

####################
# System Information
####################

  dozzle:
    container_name: dozzle
    image: amir20/dozzle:latest
    profiles:
      - monitor

    depends_on:
      - socky_proxy

    networks:
      - socky_proxy-net

    ports:
      - ${PORT_DOZZLE}:8080

    environment:
      DOCKER_HOST: tcp://socky_proxy:${PORT_SOCKY_PROXY} # redirects socket to the proxy
    
    labels:
      - diun.enable=true
      - homepage.group=System
      - homepage.name=Dozzle
      - homepage.icon=dozzle
      - homepage.href=http://${SERVER_URL}:${PORT_DOZZLE}
      - homepage.description=Docker Logs


    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 128M
    restart: always

  scrutiny:
    container_name: scrutiny
    image: ghcr.io/analogj/scrutiny:master-omnibus
    profiles:
      - monitor

    ports:
      - ${PORT_SCRUTINY}:8080 # webapp
      - ${PORT_SCRUTINY_DB}:8086 # influxDB admin

    labels:
      - diun.enable=true
      - homepage.group=System
      - homepage.name=Scrutiny
      - homepage.icon=scrutiny
      - homepage.href=http://${SERVER_URL}:${PORT_SCRUTINY}
      - homepage.description=Harddrive Health Monitoring
      - homepage.widget.type=scrutiny
      - homepage.widget.url=http://${SERVER_URL}:${PORT_SCRUTINY}

    devices:
      - /dev/sda
      - /dev/sdb
      - /dev/sdc
    
    volumes:
      - /run/udev:/run/udev:ro
      - ${CONFIGDIR}/scrutiny:/opt/scrutiny/config
      - ${DBDIR}/scrutiny:/opt/scrutiny/influxdb

    cap_add:
      - SYS_RAWIO #necessary to allow smartctl permission to query your device SMART data 
      - SYS_ADMIN #necessary for NVMe drives

    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 256M
    restart: unless-stopped


####################
# Security
####################

  crowdsec:
    container_name: crowdsec
    image: crowdsecurity/crowdsec:v1.5.2
    profiles:
      - external

    depends_on:
      - socky_proxy
    
    networks:
      - caddy-net # provides access for the reverse proxy
      - socky_proxy-net

    ports:
      - ${PORT_CROWDSEC_API}:8080 # exposes a REST API for bouncers, cscli and communication between crowdsec agent and local api
      - ${PORT_CROWDSEC_METRICS}:6060 #exposes prometheus metrics on /metrics and pprof debugging metrics on /debug

    environment:
      TZ: ${TZ}
      GID: ${PGID}
      COLLECTIONS: "crowdsecurity/linux crowdsecurity/caddy crowdsecurity/linux-lpe timokoessler/uptime-kuma LePresidente/jellyseerr crowdsecurity/pgsql crowdsecurity/endlessh schiz0phr3ne/sonarr schiz0phr3ne/radarr schiz0phr3ne/prowlarr"
      DOCKER_HOST: tcp://socky_proxy:${PORT_SOCKY_PROXY} # redirects socket to the proxy. Needed for the config.yaml direction to scan other docker containers

    volumes:
      - ${STATICCONFIGDIR}/crowdsec/acquis.yaml:/etc/crowdsec/acquis.yaml:ro
      - crowdsec-config:/etc/crowdsec/
      - crowdsec-db:/var/lib/crowdsec/data/
    #  - ${CONFIGDIR}/crowdsec:/etc/crowdsec/ # This is my preferred way to get things working, but currently doesn't work

      ## Log Files
      - /var/log/auth.log:/logs/auth.log:ro
      - /var/log/syslog.log:/logs/syslog.log:ro
      
    labels:
      - diun.enable=true
      - homepage.group=Wild West
      - homepage.name=Crowdsec
      - homepage.icon=crowdsec
      - homepage.href=https://app.crowdsec.net/login?next=/instances
      - homepage.description=Crowd-sourced Intrusion Prevention


    healthcheck:
      test: ["CMD", "cscli", "version"]

    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 128M
    restart: always

  endlessh:
    image: ghcr.io/linuxserver/endlessh:latest
    container_name: endlessh
    profiles:
      - external

    ports:
      - ${PORT_SSHPROTECT:-22}:2222
    
    environment:
      PUID: ${PUID}
      PGID: ${PGID}
      TZ: ${TZ}
      MSDELAY: ${ESSH_MSDELAY:-10000} #optional
      MAXLINES: ${ESSH_MAXLINES:-32} #optional
      MAXCLIENTS: ${ESSH_MAXCLIENTS:-4096} #optional
      LOGFILE: ${DEBUG_LOG:-false} #optional. By default, the app logs to container log. If this is set to true, the log will be output to file under /config/logs/endlessh (/config needs to be mapped).
      BINDFAMILY: ${ESSH_BINDFAMILY} #optional

    #volumes:
    #  - ${CONFIGDIR}/endlessh:/config #Required if LOGFILE is set to true.

    labels:
      - diun.enable=true
      - homepage.group=Wild West
      - homepage.name=EndlessH
      - homepage.icon=endlessh
      - homepage.description=SSH Tarpit

    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 32M  
    restart: unless-stopped


####################
# Docker Environment
####################

volumes:
  crowdsec-config:
    external: true
  crowdsec-db:
    external: true

networks:
  web:
    name: web
    external: true
  caddy-net:
    name: caddy-net
    external: true
    
  htpc-net:
    name: htpc-net
  tandoor-net:
    name: tandoor-net
  rallly-net:
    name: rallly-net
  paperless-net:
    name: paperless-net

  dns-net:
    name: dns-net
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16

  socky_proxy-net:
    name: socky_proxy-net
