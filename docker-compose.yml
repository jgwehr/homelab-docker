version: "3.9"

services:
  caddy:
    container_name: caddy
    build:
      context: ${DOCKERDIR}
      dockerfile: caddy.dockerfile #Custom build which uses duckdns

    depends_on:
      - crowdsec
      - jellyfin
      - jellyseerr
      - tandoor
      - uptime-kuma

    external_links:
      - crowdsec:crowdsec
      - jellyfin:jellyfin
      - jellyseerr:jellyseerr
      - tandoor:tandoor
      - uptime_kuma:uptime_kuma

    networks:
      - web
      - caddy-net

    ports:
      - ${PORT_CADDY_HTTP}:80
      - ${PORT_CADDY_HTTPS}:443

    environment:
      LOG_FILE: ${LOGDIR}/caddy/access.log
      DOMAIN: ${DOMAIN}
      EMAIL: ${EMAIL_ADMIN}
      DUCKDNS_API_TOKEN: ${DUCKDNS_TOKEN}
      
      BOUNCER_CADDY_TOKEN: ${BOUNCER_CADDY_TOKEN}

    labels:
      - "diun.enable=true"

    volumes:
      - ${STATICONFIGDIR}/caddy/Caddyfile:/etc/caddy/Caddyfile:ro # Required. Needs to be an extension-less file NOT a directory
      - ${CONFIGDIR}:/data # Optional, house for certs. Caddy adds its own /caddy/ directory
      - ${CONFIGDIR}:/config # Optional, JSON Config files. Caddy adds its own /caddy/ directory
      - ${STATICDIR}/tandoor_media:/www/tandoor:ro # recipe image files for Tandoor

    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 64M
    healthcheck:
      test: ["CMD", "caddy", "version"]
    restart: unless-stopped

  heimdall:
    container_name: heimdall
    image: ghcr.io/linuxserver/heimdall
    
    ports:
      - ${PORT_DASH_HTTP}:80
      - ${PORT_DASH_HTTPS}:443

    environment:
      PUID: ${PUID}
      PGID: ${PGID}
      TZ: ${TZ}

    volumes:
      - ${CONFIGDIR}/heimdall:/config
    
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 128M
    restart: unless-stopped

  uptime-kuma:
    container_name: uptime_kuma
    image: louislam/uptime-kuma
    
    networks:
      - caddy-net # provides access for the reverse proxy
    
    ports:
      - ${PORT_UPKUMA}:3001
    
    labels:
      - "diun.enable=true"

    volumes:
      - ${CONFIGDIR}/uptime-kuma:/app/data
    
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 128M
    restart: always

####################
# Recipes
####################

  tandoor:
    container_name: tandoor
    image: vabene1111/recipes:latest

    depends_on:
      tandoor_db:
        condition: service_healthy
    
    networks:
      - caddy-net # provides access for the reverse proxy
      - tandoor

    ports:
      - ${PORT_TANDOOR}:8080

    env_file:
      - ${STATICONFIGDIR}/tandoor/tandoor.env

    environment:
      TIMEZONE: ${TZ}
      POSTGRES_HOST: ${TANDOOR_DB_NAME}
      POSTGRES_USER: ${TANDOOR_DB_USER}

    labels:
      - "diun.enable=true"

    volumes:
      - ${STATICDIR}/tandoor_media:/opt/recipes/mediafiles

    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 512M
    restart: unless-stopped

  tandoor_db:
    container_name: tandoor_db
    image: postgres:11-alpine

    networks:
      - tandoor

    env_file:
      - ${STATICONFIGDIR}/tandoor/tandoor.env

    volumes:
      - ${DBDIR}/tandoor_db:/var/lib/postgresql/data

    healthcheck:
      interval: 60s
      retries: 10
      test:
        - CMD
        - pg_isready
        - '-q'
        - '-d'
        - ${TANDOOR_DB_NAME}
        - '-U'
        - ${TANDOOR_DB_USER}
      timeout: 45s
      
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 128M
    restart: unless-stopped


####################
# System Information
####################

  dozzle:
    container_name: dozzle
    image: amir20/dozzle:latest

    depends_on:
      - socky_proxy

    networks:
      - socky_proxy-net

    ports:
      - ${PORT_DOZZLE}:8080

    environment:
      DOCKER_HOST: tcp://socky_proxy:${PORT_SOCKY_PROXY} # redirects socket to the proxy
    
    labels:
      - "diun.enable=true"

    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 64M
    restart: always

####################
# Media
####################

  jellyfin:
    container_name: jellyfin
    image: jellyfin/jellyfin

    user: ${PUID}:${PGID}
    group_add:
      - ${GID_HARDWAREACC} # gid of your `render` group

    networks:
      - htpc-net
      - caddy-net # provides access for the reverse proxy
    
    ports:
      - ${PORT_HTPC_HTTP}:8096 #HTTP webUI
      - ${PORT_HTPC_HTTPS}:8920 #HTTPS webUI
      - ${PORT_HTPC_LOCAL}:7359/udp #optional. Allows clients to discover Jellyfin on the local network.
      - ${PORT_HTPC_DLNA}:1900/udp #optional. Service discovery used by DNLA and clients.

    environment:
      TZ: ${TZ}
      JELLYFIN_PublishedServerUrl: ${JELLYFIN_URL} #optional. The Server URL to publish in udp Auto Discovery response.

    labels:
      - "diun.enable=true"

    devices:
      - /dev/dri/renderD128:/dev/dri/renderD128 # VAAPI
      - /dev/dri/card0:/dev/dri/card0

    volumes:
      - ${CONFIGDIR}/jellyfin:/config
      - ${CACHEDIR}/jellyfin:/cache
      - ${MEDIADIR}:/media:ro #naming, etc all handled by *arr apps
    
    #deploy:
    #  resources:
    #    limits:
    #      cpus: '2'
    #      memory: 1024M
    restart: unless-stopped
  
  jellyseerr:
    container_name: jellyseerr
    image: fallenbagel/jellyseerr:latest
       
    networks:
      - htpc-net
      - caddy-net # provides access for the reverse proxy

    ports:
      - ${PORT_HTPC_REQUESTS}:5055

    environment:
      LOG_LEVEL: ${LOG_LEVEL}
      TZ: ${TZ}

    labels:
      - "diun.enable=true"

    volumes:
        - ${CONFIGDIR}/jellyseerr:/app/config
    
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 256M
    restart: unless-stopped
  
  qbittorrent:
    container_name: qbittorrent
    image: linuxserver/qbittorrent

    depends_on:
      protonvpn:
        condition: service_healthy

    network_mode: service:protonvpn # ONLY provide network config in the protonvpn container

    environment:
        PUID: ${PUID}
        PGID: ${PGID}
        TZ: ${TZ}
        UMASK_SET: 022
        WEBUI_PORT: ${PORT_TORRENT_UI}

    labels:
      - "diun.enable=true"

    volumes:
        - ${CONFIGDIR}/qbt:/config
        - ${DOWNLOADDIR}:/data/downloads
    
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 128M
    healthcheck:
      test: ls /sys/class/net/ | grep -o "eth[0-9*]|wg[0-9*]" > /dev/null || exit 1 # VPN Killswitch. Check to see if the wg0 interface is available, or fail. the protonvpn container supplies wg0.
      interval: 30s
      timeout: 6s
      retries: 3
      start_period: 20s
    restart: unless-stopped

  prowlarr:
    container_name: prowlarr
    image: lscr.io/linuxserver/prowlarr:develop

    depends_on:
      - protonvpn
      - qbittorrent

    network_mode: service:protonvpn # ONLY provide network config in the protonvpn container

    environment:
      PUID: ${PUID}
      PGID: ${PGID}
      TZ: ${TZ}

    labels:
      - "diun.enable=true"
      
    volumes:
      - ${CONFIGDIR}/prowlarr:/config

    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 128M
    restart: unless-stopped
  
  radarr:
    container_name: radarr
    image: lscr.io/linuxserver/radarr:latest
    
    depends_on:
      - protonvpn
      - prowlarr
      - qbittorrent

    network_mode: service:protonvpn # ONLY provide network config in the protonvpn container

    environment:
      PUID: ${PUID}
      PGID: ${PGID}
      TZ: ${TZ}

    labels:
      - "diun.enable=true"

    volumes:
        - /etc/localtime:/etc/localtime:ro
        - ${CONFIGDIR}/radarr3:/config
        - ${DATADIR}:/data
    
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 256M
    restart: unless-stopped

  sonarr:
    container_name: sonarr
    image: lscr.io/linuxserver/sonarr:latest

    depends_on:
      - protonvpn
      - prowlarr
      - qbittorrent

    network_mode: service:protonvpn # ONLY provide network config in the protonvpn container

    environment:
      PUID: ${PUID}
      PGID: ${PGID}
      TZ: ${TZ}

    labels:
      - "diun.enable=true"

    volumes:
        - /etc/localtime:/etc/localtime:ro
        - ${CONFIGDIR}/sonarr:/config
        - ${DATADIR}:/data
    
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 256M
    restart: unless-stopped


####################
# Back End Services
####################

  duckdns:
    container_name: duckdns
    image: ghcr.io/linuxserver/duckdns

    environment:
      PUID: ${PUID}
      PGID: ${PGID}
      TZ: ${TZ}
      SUBDOMAINS: ${DUCKDNS_SUBDOMAINLIST}
      TOKEN: ${DUCKDNS_TOKEN}

    volumes:
      - ${CONFIGDIR}/duckdns:/config #optional

    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 32M
    restart: unless-stopped

  diun:
    container_name: diun
    image: ghcr.io/crazy-max/diun:4.22

    depends_on:
      - socky_proxy

    networks:
      - socky_proxy-net
      
    command: serve

    environment:
      TZ: ${TZ}
      LOG_LEVEL: ${LOG_LEVEL:-info}
      LOG_JSON: "true"
      DIUN_NOTIF_DISCORD_WEBHOOKURL: ${DIUN_NOTIF_DISCORD_WEBHOOKURL}
      DIUN_PROVIDERS_DOCKER_ENDPOINT: tcp://socky_proxy:${PORT_SOCKY_PROXY} # redirects socket to the proxy

    labels:
      - "diun.enable=true"
    
    volumes:
      - ${STATICONFIGDIR}/diun/diun.yml:/diun.yml:ro
      - ${CONFIGDIR}/diun:/data

    healthcheck:
      test: ["CMD", "diun", "--version"]

    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 32M
    restart: unless-stopped

  protonvpn:
    container_name: protonvpn
    image: lscr.io/linuxserver/wireguard:latest
    
    environment:
      PUID: ${PUID}
      PGID: ${PGID}
      TZ: ${TZ}
    
    networks:
      - peer2peer
      - vpn

    ports:
      - ${PORT_WIREGUARD_OUTBOUND}:51820/udp
      - ${PORT_TORRENT_UI}:${PORT_TORRENT_UI} #default 8080
      - ${PORT_PROWLARR_UI}:9696
      - ${PORT_LIDARR_UI}:8686
      - ${PORT_RADARR_UI}:7878
      - ${PORT_SONARR_UI}:8989

    labels:
      - "diun.enable=true"

    volumes:
      - ${CONFIGDIR}/wireguard:/config
      - /lib/modules:/lib/modules:ro

    cap_add:
      - NET_ADMIN
      - SYS_MODULE
    sysctls:
      - net.ipv4.conf.all.src_valid_mark=1

    healthcheck:
      test: ["CMD", "wg", "show"]

    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 64M
    restart: unless-stopped

  socky_proxy:
    container_name: socky_proxy
    image: ghcr.io/tecnativa/docker-socket-proxy:0.1 # A security-enhanced proxy for the Docker Socket.

    privileged: true
  
    networks:
      - socky_proxy-net

    ports:
      - ${PORT_SOCKY_PROXY}:2375

    environment:
      # Variables match the URL prefix (i.e. AUTH blocks access to /auth/* parts of the API, etc).
      #   0 - revoke access
      #   1 - grant access
      CONTAINERS: 1 # crowdsec, diun
      IMAGES: 1 # diun
      INFO: 1 # crowdsec

    labels:
      - "diun.enable=true"

    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
    
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 32M

    restart: always


####################
# Security
####################

  crowdsec:
    container_name: crowdsec
    image: crowdsecurity/crowdsec:v1.4.1

    depends_on:
      - socky_proxy
    
    networks:
      - caddy-net # provides access for the reverse proxy
      - socky_proxy-net

    ports:
      - ${PORT_CROWDSEC_API}:8080 # exposes a REST API for bouncers, cscli and communication between crowdsec agent and local api
      - ${PORT_CROWDSEC_METRICS}:6060 #exposes prometheus metrics on /metrics and pprof debugging metrics on /debug

    environment:
      TZ: ${TZ}
      GID: ${PGID}
      COLLECTIONS: "crowdsecurity/linux crowdsecurity/caddy crowdsecurity/linux-lpe timokoessler/uptime-kuma LePresidente/jellyseerr crowdsecurity/pgsql crowdsecurity/endlessh schiz0phr3ne/sonarr schiz0phr3ne/radarr schiz0phr3ne/prowlarr"
      DOCKER_HOST: tcp://socky_proxy:${PORT_SOCKY_PROXY} # redirects socket to the proxy. Needed for the config.yaml direction to scan other docker containers

    volumes:
      - ${STATICONFIGDIR}/crowdsec/acquis.yaml:/etc/crowdsec/acquis.yaml:ro
      - crowdsec-config:/etc/crowdsec/
      - crowdsec-db:/var/lib/crowdsec/data/
    #  - ${CONFIGDIR}/crowdsec:/etc/crowdsec/ # This is my preferred way to get things working, but currently doesn't work

      ## Log Files
      - /var/log/auth.log:/logs/auth.log:ro
      - /var/log/syslog.log:/logs/syslog.log:ro
      
    labels:
      - "diun.enable=true"

    healthcheck:
      test: ["CMD", "cscli", "version"]

    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 128M
    restart: always

  endlessh:
    image: lscr.io/linuxserver/endlessh:latest
    container_name: endlessh

    ports:
      - ${PORT_SSHPROTECT:-22}:2222
    
    environment:
      PUID: ${PUID}
      PGID: ${PGID}
      TZ: ${TZ}
      MSDELAY: ${ESSH_MSDELAY:-10000} #optional
      MAXLINES: ${ESSH_MAXLINES:-32} #optional
      MAXCLIENTS: ${ESSH_MAXCLIENTS:-4096} #optional
      LOGFILE: ${DEBUG_LOG:-false} #optional. By default, the app logs to container log. If this is set to true, the log will be output to file under /config/logs/endlessh (/config needs to be mapped).
      BINDFAMILY: ${ESSH_BINDFAMILY} #optional

    #volumes:
    #  - ${CONFIGDIR}/endlessh:/config #Required if LOGFILE is set to true.

    labels:
      - "diun.enable=true"

    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 32M  
    restart: unless-stopped


####################
# Docker Environment
####################

volumes:
  crowdsec-config:
    external: true
  crowdsec-db:
    external: true

networks:
  web:
    name: web
    external: true
  caddy-net:
    name: caddy-net
    external: true
    

  htpc-net:
    name: htpc-net
  tandoor:
    name: tandoor

  peer2peer:
    name: peer2peer
  vpn:
    name: vpn
    internal: true

  socky_proxy-net:
    name: socky_proxy-net
